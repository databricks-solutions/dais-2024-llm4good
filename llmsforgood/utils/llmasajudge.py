import mlflow
from mlflow.metrics.genai.metric_definitions import answer_correctness, answer_relevance
from mlflow.metrics.genai import make_genai_metric, EvaluationExample


class Judge:
    """
    A class to evaluate language models (LLMs) based on predefined or custom metrics.

    Attributes:
        deploy_client: A client object for deploying models or interacting with deployment services.
    """

    def __init__(self, deploy_client):
        """
        Initializes the Judge class with a deployment client.

        Args:
            deploy_client: The deployment client to be used for model deployment and interaction.
        """
        self.deploy_client = deploy_client

    @staticmethod
    def create_evaluation_example(input, output, score, justification):
        """
        Creates an EvaluationExample instance, which represents a structured evaluation
        example including input, output, score, and a justification for the score.

        Args:
            input (str): The input text or query given to the model.
            output (str): The output text or response generated by the model.
            score (float): A numerical score assessing the quality of the output.
            justification (str): A textual explanation or justification for the given score.

        Returns:
            EvaluationExample: An instance of EvaluationExample encapsulating the evaluation data.
        """
        return EvaluationExample(
            input=input, output=output, score=score, justification=justification
        )

    @staticmethod
    def create_genai_metric(
        name,
        definition,
        grading_prompt,
        endpoint_name,
        parameters,
        aggregations,
        examples,
        greater_is_better,
    ):
        """
        Creates a custom GenAI metric for evaluating language models, using various parameters
        to define the evaluation criteria.

        Args:
            name (str): The name of the metric.
            definition (function): The function defining how the metric is calculated.
            grading_prompt (str): A prompt used for grading or guiding the evaluation process.
            endpoint_name (str): The name of the model endpoint to be evaluated.
            parameters (dict): Additional parameters for the metric calculation.
            aggregations (list): A list of aggregation functions to be applied to metric results.
            examples (list): A list of EvaluationExample instances to guide or inform the metric.
            greater_is_better (bool): A flag indicating whether higher metric values indicate better performance.

        Returns:
            A metric object created using the `make_genai_metric` function.
        """
        return make_genai_metric(
            name=name,
            definition=definition,
            grading_prompt=grading_prompt,
            model=f"endpoints:/{endpoint_name}",
            parameters=parameters,
            aggregations=aggregations,
            examples=examples,
            greater_is_better=greater_is_better,
        )

    def evaluate_llm(self, eval_df, input_column, target_column, metrics):
        """
        Evaluates a language model using a DataFrame of evaluation data and a set of metrics.

        Args:
            eval_df (DataFrame): A pandas DataFrame containing the evaluation data.
            run_name (str): The name of the MLflow run under which the evaluation will be recorded.
            input_column (str): The name of the DataFrame column containing input texts.
            target_column (str): The name of the DataFrame column containing target or expected outputs.
            metrics (list): A list of metric objects or definitions to be used in the evaluation.

        Returns:
            A DataFrame containing the evaluation results.

        Raises:
            Exception: If an error occurs during the evaluation process.
        """
        try:
            eval_df = self._prepare_data(eval_df, input_column, target_column)
            return self._run_evaluation(eval_df, metrics)
        except Exception as e:
            # Handle or log the exception
            raise

    @staticmethod
    def _prepare_data(df, input_column, target_column):
        """
        Prepares the evaluation DataFrame by renaming columns to the expected "inputs" and "targets".

        Args:
            df (DataFrame): The original pandas DataFrame containing evaluation data.
            input_column (str): The name of the column to be renamed to "inputs".
            target_column (str): The name of the column to be renamed to "targets".

        Returns:
            DataFrame: The DataFrame with renamed columns.
        """
        return df.rename(columns={input_column: "inputs", target_column: "targets"})

    def _run_evaluation(self, eval_df, metrics):
        """
        Runs the evaluation of the language model using MLflow, tracking the evaluation as a run.

        This method initiates an MLflow run, where it evaluates the language model's performance
        based on the specified metrics. The evaluation leverages the MLflow evaluation API to
        compare model predictions against target outputs within the provided DataFrame. The
        evaluation results are then returned, including detailed metrics and possibly
        visualizations, depending on the configuration of the evaluation metrics.

        Args:
            eval_df (DataFrame): A pandas DataFrame containing the prepared evaluation data,
                                  specifically with 'inputs' and 'targets' columns for evaluation.

            metrics (list): A list of metric objects or definitions to be used in the evaluation.
                            These metrics define the criteria based on which the language model's
                            outputs are assessed.

        Returns:
            DataFrame: A pandas DataFrame containing the results of the evaluation. The structure
                        and contents of this DataFrame will depend on the metrics used for evaluation
                        but typically include metric names, values, and possibly additional details
                        for each evaluation metric.

        Raises:
            Exception: Propagates exceptions that may arise during the evaluation process, which
                        could be related to issues with the MLflow environment, data preparation,
                        or execution of the evaluation metrics.
        """
        eval_results = mlflow.evaluate(
            data=eval_df[["inputs", "targets"]],
            model_type="question-answering",
            predictions="targets",
            extra_metrics=metrics,
        )
        # The eval_results object contains detailed information about the evaluation,
        # structured in a way that's specific to the MLflow evaluate API. For instance,
        # eval_results.tables['eval_results_table'] might contain a table with metric
        # names and their corresponding values.

        return eval_results.tables["eval_results_table"]
