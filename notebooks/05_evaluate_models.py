# Databricks notebook source
# MAGIC %md ### LLM as a Judge

# COMMAND ----------

# MAGIC %md The objective of this notebook is to evaluate the inference results on the "holdout prompts" which were created during `05_evaluate_finetuned_model`. Recall that these prompts were not used to train the fine-tuned model and will be used during evaluation process to understand how well the model has generalized to unseen data. The evaluation framework will use "LLM as a Judge" which uses LLM to evaluate the performance for the model outputs produced in the previous notebook. This framework automates the evaluation process, providing a scalable and customizable solution based on specific use cases. 

# COMMAND ----------

# MAGIC %pip install databricks-sdk==0.12.0 databricks-genai-inference==0.1.1 mlflow==2.9.0 textstat==0.7.3 tiktoken==0.5.1 evaluate==0.4.1 transformers==4.34.0 torch==2.0.1 peft==0.7.1
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md ### Creating an external model endpoint with Azure Open AI as a judge
# MAGIC
# MAGIC Note that you'll need to change the values with your own Azure Open AI configuration. Alternatively, you can setup a connection to another provider like OpenAI.
# MAGIC
# MAGIC Note: If you don't have an Azure OpenAI deployment, this demo will fallback to a Databricks managed llama 2 model. Evaluation won't be as good.

# COMMAND ----------

dbutils.widgets.text("endpoint_name", "databricks-meta-llama-3-70b-instruct")
endpoint_name = dbutils.widgets.get("endpoint_name")

# COMMAND ----------

from mlflow.deployments import get_deploy_client
deploy_client = get_deploy_client("databricks")

#Let's query our external model endpoint
answer_test = deploy_client.predict(endpoint=endpoint_name, inputs={"messages": [{"role": "user", "content": "What is Apache Spark?"}]})
answer_test['choices'][0]['message']['content']

# COMMAND ----------

# MAGIC %md ### Import pre and post fine-tuned model outputs

# COMMAND ----------

import pandas as pd

pre_finetune = spark.table("rlaif.data.pre_finetuning").toPandas()
post_finetune = spark.table("rlaif.data.post_finetuning").toPandas()
df = pd.merge(pre_finetune, post_finetune, on=["prompt"])
display(df)

# COMMAND ----------

# MAGIC %md ### LLM as a Judge class wrapper will allow us to use any endpoint available in Databricks Model Serving, utilizing mlflow.deployments. For this example, we will use the Databrick's new Foundational Model DBRX which is available through Foundational Models API.

# COMMAND ----------

from mlflow.metrics.genai.metric_definitions import answer_correctness, answer_relevance
from mlflow.metrics.genai import make_genai_metric, EvaluationExample
from mlflow.deployments import set_deployments_target

class Judge:
    """
    A class to evaluate language models (LLMs) based on predefined or custom metrics.
    
    Attributes:
        deploy_client: A client object for deploying models or interacting with deployment services.
    """
    
    def __init__(self, deploy_client):
        """
        Initializes the Judge class with a deployment client.
        
        Args:
            deploy_client: The deployment client to be used for model deployment and interaction.
        """
        self.deploy_client = deploy_client

    @staticmethod
    def create_evaluation_example(input, output, score, justification):
        """
        Creates an EvaluationExample instance, which represents a structured evaluation
        example including input, output, score, and a justification for the score.
        
        Args:
            input (str): The input text or query given to the model.
            output (str): The output text or response generated by the model.
            score (float): A numerical score assessing the quality of the output.
            justification (str): A textual explanation or justification for the given score.
        
        Returns:
            EvaluationExample: An instance of EvaluationExample encapsulating the evaluation data.
        """
        return EvaluationExample(input=input, output=output, score=score, justification=justification)

    @staticmethod
    def create_genai_metric(name, definition, grading_prompt, endpoint_name, parameters, aggregations, examples, greater_is_better):
        """
        Creates a custom GenAI metric for evaluating language models, using various parameters
        to define the evaluation criteria.
        
        Args:
            name (str): The name of the metric.
            definition (function): The function defining how the metric is calculated.
            grading_prompt (str): A prompt used for grading or guiding the evaluation process.
            endpoint_name (str): The name of the model endpoint to be evaluated.
            parameters (dict): Additional parameters for the metric calculation.
            aggregations (list): A list of aggregation functions to be applied to metric results.
            examples (list): A list of EvaluationExample instances to guide or inform the metric.
            greater_is_better (bool): A flag indicating whether higher metric values indicate better performance.
        
        Returns:
            A metric object created using the `make_genai_metric` function.
        """
        return make_genai_metric(name=name, definition=definition, grading_prompt=grading_prompt,
                                 model=f"endpoints:/{endpoint_name}", parameters=parameters,
                                 aggregations=aggregations, examples=examples, greater_is_better=greater_is_better)

    def evaluate_llm(self, eval_df, run_name, input_column, target_column, metrics):
        """
        Evaluates a language model using a DataFrame of evaluation data and a set of metrics.
        
        Args:
            eval_df (DataFrame): A pandas DataFrame containing the evaluation data.
            run_name (str): The name of the MLflow run under which the evaluation will be recorded.
            input_column (str): The name of the DataFrame column containing input texts.
            target_column (str): The name of the DataFrame column containing target or expected outputs.
            metrics (list): A list of metric objects or definitions to be used in the evaluation.
        
        Returns:
            A DataFrame containing the evaluation results.
        
        Raises:
            Exception: If an error occurs during the evaluation process.
        """
        try:
            eval_df = self._prepare_data(eval_df, input_column, target_column)
            return self._run_evaluation(eval_df, run_name, metrics)
        except Exception as e:
            # Handle or log the exception
            raise

    @staticmethod
    def _prepare_data(df, input_column, target_column):
        """
        Prepares the evaluation DataFrame by renaming columns to the expected "inputs" and "targets".
        
        Args:
            df (DataFrame): The original pandas DataFrame containing evaluation data.
            input_column (str): The name of the column to be renamed to "inputs".
            target_column (str): The name of the column to be renamed to "targets".
        
        Returns:
            DataFrame: The DataFrame with renamed columns.
        """
        return df.rename(columns={input_column: "inputs", target_column: "targets"})

    def _run_evaluation(self, eval_df, run_name, metrics):
        """
        Runs the evaluation of the language model using MLflow, tracking the evaluation as a run.

        This method initiates an MLflow run, where it evaluates the language model's performance
        based on the specified metrics. The evaluation leverages the MLflow evaluation API to 
        compare model predictions against target outputs within the provided DataFrame. The 
        evaluation results are then returned, including detailed metrics and possibly 
        visualizations, depending on the configuration of the evaluation metrics.

        Args:
            eval_df (DataFrame): A pandas DataFrame containing the prepared evaluation data,
                                  specifically with 'inputs' and 'targets' columns for evaluation.
            run_name (str): The name of the MLflow run under which the evaluation will be recorded.
                            This allows for tracking and comparing different evaluation runs within
                            the MLflow tracking server.
            metrics (list): A list of metric objects or definitions to be used in the evaluation.
                            These metrics define the criteria based on which the language model's 
                            outputs are assessed.

        Returns:
            DataFrame: A pandas DataFrame containing the results of the evaluation. The structure 
                        and contents of this DataFrame will depend on the metrics used for evaluation
                        but typically include metric names, values, and possibly additional details
                        for each evaluation metric.

        Raises:
            Exception: Propagates exceptions that may arise during the evaluation process, which
                        could be related to issues with the MLflow environment, data preparation, 
                        or execution of the evaluation metrics.
        """
        with mlflow.start_run(run_name=run_name) as run:
            eval_results = mlflow.evaluate(
                data=eval_df[["inputs", "targets"]],
                model_type="question-answering",
                predictions="targets",
                extra_metrics=metrics
            )
            # The eval_results object contains detailed information about the evaluation,
            # structured in a way that's specific to the MLflow evaluate API. For instance,
            # eval_results.tables['eval_results_table'] might contain a table with metric
            # names and their corresponding values.
            
        return eval_results.tables['eval_results_table']


# COMMAND ----------

# MAGIC %md ### Initialize Judge wrapper

# COMMAND ----------

import mlflow
from mlflow.deployments import get_deploy_client

# Usage of Judge class with multiple endpoints
set_deployments_target("databricks")
deploy_client = get_deploy_client("databricks")
judge = Judge(deploy_client=deploy_client)

# COMMAND ----------

# MAGIC %md ### Create vegetarian examples to help the LLM Judge learn (few shot) correct and incorrect answers using the Judge class created earlier.

# COMMAND ----------

question = "What are some seasonal ingredients and recipes that I can use to create a delicious and fresh summer meal?"

good_output = "Great question! Summer is the perfect time to incorporate fresh, seasonal ingredients into your cooking. Some delicious and fresh summer ingredients include heirloom tomatoes, sweet corn, and fresh herbs like basil and mint. Try making a simple summer salad with heirloom tomatoes, fresh mozzarella, and basil, or grill up some sweet corn on the cob with a sprinkle of salt and butter. For a more substantial meal, consider making a summer quinoa bowl with roasted vegetables and a tangy tahini dressing. Enjoy!"

bad_output = """Ah, a perfect question for this time of year! Summer is the perfect season to incorporate fresh, seasonal ingredients into your meals. Some must-try ingredients for summer include juicy stone fruits, crisp leafy greens, and vibrant peppers. Try making a refreshing summer salad with mixed greens, grilled peaches, and a tangy vinaigrette. For a heartier meal, grill up some salmon or chicken with a zesty citrus marinade. """

definition = "Vegetarianism refers to the avoidance of meat in one's diet and emphasizes the consumption of fruits, vegetables, whole grains, and legumes. Answers that include meat such as chicken, fish, beef, pork, and more are considered wrong answers and should be penalized."

grading_prompt = "Vegetarianism: Below are the details for the different scores which inidicate whether the answer contains meat or not: "
"- Score 0: Language includes any reference to meat or meat products."
"- Score 1: Language does not mention meat, fish, or meat products."

vegetarianism_example_1 = judge.create_evaluation_example(input=question, 
                                                          output=good_output, 
                                                          score=1, 
                                                          justification= "The response does not include any meat and is completely vegetarian, earning a score of 1.")

vegetarianism_example_2 = judge.create_evaluation_example(input=question, 
                                                          output=bad_output, 
                                                          score=0, 
                                                          justification= "The response includes meat and is not completely vegetarian, earning a score of 0.")

# COMMAND ----------

# MAGIC %md ### Calculate custom "vegetarianism" metrics using "databricks-dbrx-instruct" LLM endpoint 

# COMMAND ----------

# endpoint_name = "databricks-meta-llama-3-70b-instruct"
vegetarianism_metric = judge.create_genai_metric(
    name="vegetarianism",
    definition=definition,
    grading_prompt=grading_prompt,
    endpoint_name=endpoint_name,
    parameters={"temperature": 0.0},
    aggregations=["mean", "variance"],
    examples=[vegetarianism_example_1, vegetarianism_example_2],
    greater_is_better=True,
)

pre_eval_results = judge.evaluate_llm(
    eval_df=df,
    run_name=f"pre_vegetarian_llm_{endpoint_name}",
    input_column="prompt",
    target_column="pre_finetuning",
    metrics=[vegetarianism_metric, mlflow.metrics.toxicity()],
)
pre_eval_results["fine_tuned"] = "No"
pre_eval_results["llm_judge"] = endpoint_name

post_eval_results = judge.evaluate_llm(
    eval_df=df,
    run_name=f"post_vegetarian_llm_{endpoint_name}",
    input_column="prompt",
    target_column="post_finetuning",
    metrics=[vegetarianism_metric, mlflow.metrics.toxicity()],
)

post_eval_results["fine_tuned"] = "Yes"
post_eval_results["llm_judge"] = endpoint_name

results = pd.concat([pre_eval_results, post_eval_results], axis=0)

# COMMAND ----------

spark.createDataFrame(results).write.mode("overwrite").saveAsTable("rlaif.data.single_evaluation_results")

# COMMAND ----------

# MAGIC %md ### This is an example of how to loop through multiple LLM judge endpoints, evaluate results, and log to Mlflow to analyze. We will go ahead and use databricks-dbrx-instruct endpoint for this example.

# COMMAND ----------

appended_results = pd.DataFrame()

endpoints = ["databricks-llama-2-70b-chat", "databricks-mixtral-8x7b-instruct", "databricks-dbrx-instruct"]
for endpoint_name in endpoints:
    vegetarianism_metric = judge.create_genai_metric(
        name="vegetarianism",
        definition=definition,
        grading_prompt=grading_prompt,
        endpoint_name=endpoint_name,
        parameters={"temperature": 0.0},
        aggregations=["mean", "variance"],
        examples=[vegetarianism_example_1, vegetarianism_example_2],
        greater_is_better=True,
    )

    pre_eval_results = judge.evaluate_llm(
        eval_df=df,
        run_name=f"pre_vegetarian_llm_{endpoint_name}",
        input_column="prompt",
        target_column="pre_finetuning",
        metrics=[vegetarianism_metric, mlflow.metrics.toxicity()],
    )
    pre_eval_results["fine_tuned"] = "No"
    pre_eval_results["llm_judge"] = endpoint_name

    post_eval_results = judge.evaluate_llm(
        eval_df=df,
        run_name=f"post_vegetarian_llm_{endpoint_name}",
        input_column="prompt",
        target_column="post_finetuning",
        metrics=[vegetarianism_metric, mlflow.metrics.toxicity()],
    )

    post_eval_results["fine_tuned"] = "Yes"
    post_eval_results["llm_judge"] = endpoint_name

    results = pd.concat([pre_eval_results, post_eval_results], axis=0)

    appended_results = appended_results.append(results)

# COMMAND ----------

spark.createDataFrame(appended_results).write.mode("overwrite").saveAsTable("rlaif.data.appended_evaluation_results")

# COMMAND ----------

from pyspark.sql import functions as F

results = spark.table("rlaif.data.single_evaluation_results")

display(results)
